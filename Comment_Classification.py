# -*- coding: utf-8 -*-
"""mp2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ZF3swfRauxkDZX5vHzBD5DrpgDBTB_fI
"""

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline
from google.colab import drive
drive.mount('/content/drive')
# %cd /content/drive/MyDrive/mini-project2

import os
import json
import math 

import scipy
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

import time
import cv2

from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import cross_val_score
from sklearn.datasets import make_classification
from sklearn.linear_model import SGDClassifier
from sklearn.pipeline import Pipeline
from sklearn.feature_extraction.text import TfidfTransformer
import re
from sklearn.preprocessing import Normalizer

train_df = pd.read_csv('train.csv', encoding='windows-1252')
test_df = pd.read_csv('test.csv', encoding='windows-1252')

test_numpy = test_df.to_numpy()

miss_vals_train = train_df.isnull().sum()
miss_vals_test = test_df.isnull().sum() 
print(miss_vals_train, miss_vals_test)

import pandas as pd
import numpy as np
from sklearn.preprocessing import LabelEncoder

LE = LabelEncoder()
train_df['label'] = LE.fit_transform(train_df['subreddit'])
train_df.head()
train_numpy = train_df.to_numpy()

"""# Find the best feature extraction method. (5-fold cross-validation is applied on all the experiments)

## a) simple vectorizer
"""

from sklearn.linear_model import LogisticRegression
from sklearn import metrics
from sklearn.preprocessing import Normalizer
normalizer_train = Normalizer()

from sklearn.feature_extraction.text import CountVectorizer
vectorizer = CountVectorizer(binary=True)
print(vectorizer)
vectors_train = vectorizer.fit_transform(train_numpy[:,0])
vectors_test = vectorizer.transform(test_numpy[:,1])
vectors_train = normalizer_train.transform(vectors_train)
vectors_test = normalizer_train.transform(vectors_test)
print(vectors_train.shape)
print(vectors_test.shape)

clf = LogisticRegression()
scores = cross_val_score(clf, vectors_train, train_numpy[:,1], cv=5)
print(scores.mean())

clf.fit(vectors_train, train_numpy[:,1])
y_pred = clf.predict(vectors_test)
y_pred_train = clf.predict(vectors_train)
metrics.accuracy_score(train_numpy[:,1], y_pred_train)

"""##b) Stop words"""

from sklearn.feature_extraction.text import CountVectorizer
from sklearn.feature_extraction import text 

stop_words = text.ENGLISH_STOP_WORDS
vectorizer = CountVectorizer(stop_words = stop_words, binary=False)
vectors_train_stop = vectorizer.fit_transform(train_numpy[:,0])
vectors_test_stop = vectorizer.transform(test_numpy[:,1])
vectors_train_stop= normalizer_train.transform(vectors_train_stop)
vectors_test_stop = normalizer_train.transform(vectors_test_stop)
print(vectors_train_stop.shape)

clf = LogisticRegression()
scores = cross_val_score(clf, vectors_train_stop, train_numpy[:,2], cv=5)
print(scores.mean())

clf.fit(vectors_train_stop, train_numpy[:,1])
y_pred = clf.predict(vectors_test_stop)
y_pred_train = clf.predict(vectors_train_stop)
metrics.accuracy_score(train_numpy[:,1], y_pred_train)

"""##c) Tf–idf


"""

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.feature_extraction import text 
from sklearn.feature_extraction import text 
import nltk
from nltk.stem.snowball import SnowballStemmer
from nltk.stem.porter import PorterStemmer
from nltk import word_tokenize          


def tokenize(text):
    tokens = nltk.word_tokenize(text)
    stems = []
    for item in tokens:
        stems.append(SnowballStemmer(language='english').stem(item))
    return stems

class StemTokenizer:
     def __init__(self):
       self.wnl =PorterStemmer()
     def __call__(self, doc):
       return [self.wnl.stem(t) for t in word_tokenize(doc) if t.isalpha()]

stop_words = text.ENGLISH_STOP_WORDS
tf_idf_vectorizer = TfidfVectorizer(stop_words = 'english', binary=True, max_features=5000, lowercase=True, ngram_range=(1,2), use_idf=True, smooth_idf=False, norm='l2')
vectors_train_idf = tf_idf_vectorizer.fit_transform(train_numpy[:,0])
vectors_test_idf = tf_idf_vectorizer.transform(test_numpy[:,1])
vectors_train_idf= normalizer_train.transform(vectors_train_idf)
vectors_test_idf = normalizer_train.transform(vectors_test_idf)

clf = LogisticRegression()
scores = cross_val_score(clf, vectors_train_idf, train_numpy[:,1], cv=5)
print(scores.mean())

"""##d) Natural Language toolkit (nltk) - Stemming"""

!pip install nltk
import nltk
nltk.download('punkt')

nltk.download('wordnet')
nltk.download('omw-1.4')

nltk.download('averaged_perceptron_tagger')

from nltk.stem import PorterStemmer
from nltk import word_tokenize          

class StemTokenizer:
     def __init__(self):
       self.wnl =PorterStemmer()
     def __call__(self, doc):
       return [self.wnl.stem(t) for t in word_tokenize(doc) if t.isalpha()]

vectorizer = CountVectorizer(tokenizer=StemTokenizer(), binary=True)
vectors_train_stem = vectorizer.fit_transform(train_numpy[:,0])
vectors_test_stem = vectorizer.transform(test_numpy[:,1])
vectors_train_stem= normalizer_train.transform(vectors_train_stem)
vectors_test_stem = normalizer_train.transform(vectors_test_stem)
print(vectors_train_stem.shape)

clf = LogisticRegression()
scores = cross_val_score(clf, vectors_train_stem, train_numpy[:,1], cv=5)
print(scores.mean())

#clf.fit(vectors_train_stem, train_numpy[:,1])
#y_pred = clf.predict(vectors_test_stem)
#y_pred_train = clf.predict(vectors_train_stem)
#metrics.accuracy_score(train_numpy[:,1], y_pred_train)

"""##e) Lemmatization"""

from nltk import word_tokenize          
from nltk.stem import WordNetLemmatizer 
from nltk.corpus import wordnet


def get_wordnet_pos(word):
    """Map POS tag to first character lemmatize() accepts"""
    tag = nltk.pos_tag([word])[0][1][0].upper()
    tag_dict = {"J": wordnet.ADJ,
                "N": wordnet.NOUN,
                "V": wordnet.VERB,
                "R": wordnet.ADV}
    return tag_dict.get(tag, wordnet.NOUN)
class New_LemmaTokenizer:
     def __init__(self):
       self.wnl = WordNetLemmatizer()
     def __call__(self, doc):
       return [self.wnl.lemmatize(t,pos =get_wordnet_pos(t)) for t in word_tokenize(doc) if t.isalpha()]

vectorizer = CountVectorizer(tokenizer=New_LemmaTokenizer(), binary=True)
vectors_train_Lemma = vectorizer.fit_transform(train_numpy[:,0])
vectors_test_Lemma = vectorizer.transform(test_numpy[:,1])
vectors_train_Lemma= normalizer_train.transform(vectors_train_Lemma)
vectors_test_Lemma = normalizer_train.transform(vectors_test_Lemma)
print(vectors_train_Lemma.shape)

clf = LogisticRegression()
scores = cross_val_score(clf, vectors_train_Lemma, train_numpy[:,1], cv=5)
print(scores.mean())

#clf.fit(vectors_train_Lemma, train_numpy[:,1])
#y_pred = clf.predict(vectors_test_Lemma)
#y_pred_train = clf.predict(vectors_train_Lemma)
#metrics.accuracy_score(train_numpy[:,1], y_pred_train)

"""So far, we have implemented several feature extraction approaches. It seems that TF-idf is the best feature extraction method for the given data set. 

# Now we try to find the best classifier for the given data set.

Function to save the best result as a csv file:
"""

def save_output(y_preds):
  header = ['id', 'subreddit']
  with open("result.csv", "w") as f:
      # 2. step
      writer = csv.writer(f)
      # 3. step
      writer.writerow(header)
      for i, pred in enumerate(list(y_preds)):
        writer.writerow([i + 1, pred])

"""Find best hyperparameters for all models below:

##a) Logistic Regression
"""

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.feature_extraction import text 

stop_words = text.ENGLISH_STOP_WORDS

import csv
tf_idf_vectorizer = TfidfVectorizer(stop_words = stop_words, binary=False, max_features=2000, lowercase=True, ngram_range=(1,1))

train = pd.read_csv('train.csv', encoding='windows-1252')
test = pd.read_csv('test.csv', encoding='windows-1252')
train_numpy = train.to_numpy()
test_numpy = test.to_numpy()


# tf_idf_vectorizer = TfidfVectorizer()
vectors_train_idf = tf_idf_vectorizer.fit_transform(train_numpy[:,0])
vectors_test_idf = tf_idf_vectorizer.transform(test_numpy[:,1])
vectors_train_idf= normalizer_train.transform(vectors_train_idf)
vectors_test_idf = normalizer_train.transform(vectors_test_idf)

clf = LogisticRegression()
scores = cross_val_score(clf, vectors_train_idf, train_numpy[:,1], cv=5)
print(scores.mean())

clf.fit(vectors_train_idf, train_numpy[:,1])
y_pred = clf.predict(vectors_test_idf)

# save_output(y_pred)
# 0.7858488733488734
# TODO: run runner on this too!

"""we examined all hyperparameters for TFIDF on logistic regression and saw that 
```
tf_idf_vectorizer 
```
variable above is the best feature extractor entity we could find.

##b) Naive Bayes BernoulliNB
"""

from sklearn.naive_bayes import BernoulliNB

alphas = [0.01, 0.05, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1, 1.25, 1.5, 1.75, 2, 2.25, 2.5, 2.75, 3, 4, 5, 6, 7, 8, 9, 10, 15, 20, 25]
best = 0
for a in alphas:
  clf = BernoulliNB(alpha = a)
  scores = cross_val_score(clf, vectors_train_idf, train_numpy[:,1], cv=5)
  if scores.mean() > best:
    best = scores.mean()
    best_comb = a

print(best.mean(), best_comb)

"""##c) Naive Bayes MultinomialNB

"""

from sklearn.naive_bayes import MultinomialNB
best = 0

alphas = [0.05, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1, 1.25, 1.5, 1.75, 2, 2.25, 2.5, 2.75, 3, 4, 5, 6, 7, 8, 9, 10, 15, 20, 25]
for alpha in alphas:
  clf = MultinomialNB(alpha=alpha)  
  scores = cross_val_score(clf, vectors_train_idf, train_numpy[:,1], cv=5)
  if scores.mean() > best:
    best = scores.mean()
    best_comb = alpha

print(best, best_comb)

"""##d) Random Forest"""

from pandas.core.groupby.groupby import com
from sklearn.ensemble import RandomForestClassifier
n_estimators = [20, 40, 100, 150, 200, 300, 500, 1000, 1500]
max_depths = [5, 10, 20, 25, 40, 50, 80, 100, 150, 200, 250]
best= 0 
combinations = []
for n_es in n_estimators:
  for max_d in max_depths:
    combinations.append([n_es, max_d])

for i, comb in enumerate(combinations):
  clf = RandomForestClassifier(n_estimators=comb[0],  max_depth=comb[1], random_state=1)
  scores = cross_val_score(clf, vectors_train_idf, train_numpy[:,1], cv=5)
  if scores.mean() > best:
    best = scores.mean()
    best_comb = comb

print(best, best_comb)
# 0.7580808080808081 [1500, 40]

from sklearn.ensemble import RandomForestClassifier

clf = RandomForestClassifier(n_estimators=1500,  max_depth=40, random_state=1)
scores = cross_val_score(clf, vectors_train_idf, train_numpy[:,1], cv=5)
print(scores.mean())

from sklearn.ensemble import ExtraTreesClassifier
best = 0
n_estimators = [20, 40, 100, 150, 200, 350, 500, 750, 1000, 1500, 2000]
max_depths = [5, 10, 20, 25, 40, 50, 80, 100, 150, 200, 250, 400]
combinations = []

combinations = []
for n_es in n_estimators:
  for max_d in max_depths:
    combinations.append([n_es, max_d])

for i, comb in enumerate(combinations):
  clf = ExtraTreesClassifier(n_estimators=comb[0], max_depth=comb[1], random_state=1)
  scores = cross_val_score(clf, vectors_train_idf, train_numpy[:,1], cv=5)
  if scores.mean() > best:
      best = scores.mean()
      best_comb = comb

print(best)
print(best_comb)
# 0.7650349650349652
# [750, 100]

"""##*e*) Decision Tree"""

from sklearn.tree import DecisionTreeClassifier

max_depths = [5, 10, 20, 25, 40, 50, 80, 100, 150, 200]
best = 0

for i, comb in enumerate(max_depths):
  clf = DecisionTreeClassifier(max_depth=comb, random_state=1)
  scores = cross_val_score(clf, vectors_train_idf, train_numpy[:,1], cv=5)
  if scores.mean() > best:
      best = scores.mean()
      best_comb = comb

print(best)
print(best_comb)
# 0.6273407148407149
# 25

"""##*f)* AdaBooost """

from pandas.core.reshape.api import lreshape
from sklearn.ensemble import AdaBoostClassifier

n_estimators = [20, 40, 100, 150, 200, 300]
learning_rate = [0.001, 0.005, 0.01, 0.05, 0.1, 0.2, 0.5, 1.0]
best= 0 
combinations = []
for n_es in n_estimators:
  for lr in learning_rate:
    combinations.append([n_es, lr])


bset = 0
for i, comb in enumerate(combinations):
  clf = AdaBoostClassifier(n_estimators=comb[0], random_state=1, learning_rate=comb[1])
  scores = cross_val_score(clf, vectors_train_idf, train_numpy[:,1], cv=5)
  if scores.mean() > best:
      best = scores.mean()
      best_comb = comb

print(best)
print(best_comb)
# 0.6717851592851594
# [300, 0.05]

"""##*g)* GradientBooosting"""

from sklearn.ensemble import GradientBoostingClassifier
best= 0
n_estimators = [50, 100, 150, 200, 300, 500, 1000]
learning_rate = [0.001, 0.005, 0.01, 0.05, 0.1, 0.2, 0.5]

combinations = []
for n_es in n_estimators:
  for lr in learning_rate:
    combinations.append([n_es, lr])
for i, comb in enumerate(combinations):
  clf = GradientBoostingClassifier(n_estimators=comb[0], learning_rate=comb[1], random_state=1)
  scores = cross_val_score(clf, vectors_train_idf, train_numpy[:,1], cv=5)
  if scores.mean() > best:
      best = scores.mean()
      best_comb = comb

print(best)
print(best_comb)

"""## *h)* SVM"""

from sklearn.svm import LinearSVC
best = 0

clf = LinearSVC(loss='hinge', C=0.4, random_state=1)
score = cross_val_score(clf, vectors_train_idf, train_numpy[:, 1], cv=5)
score.mean()

"""## Best: an ensemble of aforementioned models to obtain the best performace

According to our observation from previous experimetns we now can soft-ensemble some models with proper weights to obtain a respectable model as follow:
"""

from sklearn.ensemble import VotingClassifier
from sklearn.ensemble import ExtraTreesClassifier
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.naive_bayes import MultinomialNB
from sklearn.linear_model import LogisticRegression
from sklearn.svm import LinearSVC, SVC
from nltk.classify.scikitlearn import SklearnClassifier
import time

# using logisticregression, svm, randomforest, extraTreesClassifier, and MultiNB
clf1 = LogisticRegression(random_state=1)
clf2 = GradientBoostingClassifier(n_estimators=50, learning_rate=0.5)
clf3 = LinearSVC(loss='hinge', C=0.4, random_state=1)
clf4 = ExtraTreesClassifier(n_estimators=750, max_depth=100, random_state=1)
clf5 = RandomForestClassifier(n_estimators=1500,  max_depth=40, random_state=1)
clf6 = MultinomialNB(alpha=0.9)

# changed the ordering a bit
eclf = VotingClassifier(
       estimators=[('lr', clf1), ('svm', clf3), ('rf', clf5),('gb', clf2),  ('etc', clf4), ('mnb', clf6)],
       voting='hard')



for clf, label in zip([clf1, clf2, clf3, clf4, clf5, clf6, eclf], ['Logistic Regression', 'GradientBoosting', 'LinearSVC', 'ExtraTreesClassifier', 'RandomForestClassifier', 'MultinomialNB', 'Ensemble']):
      start_time = time.time()
      scores = cross_val_score(clf, vectors_train_idf, train_numpy[:, 1], scoring='accuracy', cv=5)
      print("Accuracy: %0.2f (+/- %0.2f) [%s] run time: %0.2f s" % (scores.mean(), scores.std(), label, time.time() - start_time))

# Accuracy: 0.81 (+/- 0.03) [Logistic Regression] run time: 0.65 s
# Accuracy: 0.74 (+/- 0.04) [GradientBoosting] run time: 6.80 s
# Accuracy: 0.79 (+/- 0.03) [LinearSVC] run time: 0.09 s
# Accuracy: 0.78 (+/- 0.04) [ExtraTreesClassifier] run time: 18.55 s
# Accuracy: 0.79 (+/- 0.03) [RandomForestClassifier] run time: 33.09 s
# Accuracy: 0.79 (+/- 0.04) [MultinomialNB] run time: 0.03 s
# Accuracy: 0.82 (+/- 0.03) [Ensemble] run time: 57.83 s

eclf.fit(vectors_train_idf, train_numpy[:,1])
y_pred = eclf.predict(vectors_test_idf)

# save results
save_output(y_pred)

"""Thus, Logistic Regression has the best performance among all other simple machine learning models.

#3) Now we implement more complex neural networks(LSTM).

# data preprocessing
"""

from sklearn.datasets import make_classification
from sklearn.linear_model import SGDClassifier
from sklearn.pipeline import Pipeline
from sklearn.feature_extraction.text import TfidfTransformer
import re
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.feature_extraction import text 
import nltk
from keras.preprocessing.text import Tokenizer
nltk.download("stopwords")
from nltk.corpus import stopwords
from keras_preprocessing.sequence import pad_sequences
from keras.callbacks import EarlyStopping
from tensorflow.keras.layers import Dropout
from tensorflow.keras.layers import Conv1D
from tensorflow.keras.layers import MaxPooling1D
from keras.models import Sequential
from keras.layers import Dense, Embedding, LSTM, SpatialDropout1D



train = pd.read_csv('train.csv', encoding='windows-1252')
test = pd.read_csv('test.csv', encoding='windows-1252')
train_numpy = train.to_numpy()
test_numpy = test.to_numpy()



REPLACE_BY_SPACE_RE = re.compile('[/(){}\[\]\|@,;]')
BAD_SYMBOLS_RE = re.compile('[^0-9a-z #+_]')
numbers = pattern = r'[0-9]'
#STOPWORDS = set(stopwords.words('english'))
STOPWORDS=set(text.ENGLISH_STOP_WORDS)

def clean_text(text):
    """
        text: a string
        
        return: modified initial string
    """
    text = text.lower() # lowercase text
    text = REPLACE_BY_SPACE_RE.sub(' ', text) # replace REPLACE_BY_SPACE_RE symbols by space in text. substitute the matched string in REPLACE_BY_SPACE_RE with space.
    #text = BAD_SYMBOLS_RE.sub('', text) # remove symbols which are in BAD_SYMBOLS_RE from text. substitute the matched string in BAD_SYMBOLS_RE with nothing. 
    #text = text.replace('x', '')
    #text = re.sub(r'\W+', '', text)
    text = re.sub(numbers, '', text)
    text = ' '.join(word for word in text.split() if word not in STOPWORDS) # remove stopwors from text
    return text

X_train = train_numpy[:,0]
X_test = test_numpy[:,1]
for i in range(len(X_train)):
  X_train[i] = clean_text(X_train[i])
  #X_train[i] = X_train[i].replace('\d+', '')

for i in range(len(X_test)):
  X_test[i] = clean_text(X_test[i])
  #X_test[i] = X_test[i].replace('\d+', '')



# The maximum number of words to be used. (most frequent)
MAX_NB_WORDS = 7000
# Max number of words in each complaint.
MAX_SEQUENCE_LENGTH = 600
# This is fixed.
EMBEDDING_DIM = 200

tokenizer = Tokenizer(num_words=MAX_NB_WORDS, filters='!"#&()*+,-./:;=?@_{|}~', lower=True)
tokenizer.fit_on_texts(X_train)
#tokenizer.fit_on_texts(X_test)
word_index = tokenizer.word_index
print('Found %s unique tokens.' % len(word_index))
print(word_index)
X = tokenizer.texts_to_sequences(X_train)
X = pad_sequences(X, maxlen=MAX_SEQUENCE_LENGTH)
print('Shape of data tensor:', X.shape)
Y = pd.get_dummies(train_numpy[:,1])



X2 = tokenizer.texts_to_sequences(X_test)
X2 = pad_sequences(X2, maxlen=MAX_SEQUENCE_LENGTH)

"""##a) Simple LSTM for Sequence Classification (run on whole data set)"""

model = Sequential()
model.add(Embedding(MAX_NB_WORDS, EMBEDDING_DIM, input_length=X.shape[1]))

model.add(SpatialDropout1D(0.2))
model.add(LSTM(200, dropout=0.2, recurrent_dropout=0.2))
model.add(Dense(4, activation='softmax'))

model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
X_tra, X_te, y_tra, y_te = train_test_split(X , Y, test_size=0.2)


#model.fit(X, Y, epochs=10, batch_size=64)
model.fit(X, Y, epochs=15, batch_size=128,validation_split=0.2)
# Final evaluation of the model

scores = model.evaluate(X_te, y_te, verbose=0)
print("Accuracy: %.2f%%" % (scores[1]*100))

pred = model.predict(X2)
pred_cat = []
for i in range(len(pred)):
  index = pred[i].argmax()
  if(int(index) == 0):
    pred_cat.append('Javascript')
  if (int(index) == 1):
    pred_cat.append('Matlab')
  if (int(index) == 2):
    pred_cat.append('Pytorch')
  if (int(index) == 3):
    pred_cat.append('Tensorflow')


print(pred_cat)

import csv

save_output(pred_cat)

"""###a-2) Simple LSTM for Sequence Classification (with cross-validation)"""

from sklearn.model_selection import KFold

kf = KFold(n_splits=5, shuffle=True, random_state=50)
kf.get_n_splits(X)
scores = 0
for train_index, test_index in kf.split(X):

  #print("TEST:", test_index)
  X_train, X_test = X[train_index], X[test_index]
  y_train, y_test = Y.loc[train_index], Y.loc[test_index]

  model = Sequential()
  model.add(Embedding(MAX_NB_WORDS, EMBEDDING_DIM, input_length=X.shape[1]))

  model.add(SpatialDropout1D(0.2))
  model.add(LSTM(200, dropout=0.2, recurrent_dropout=0.2))
  model.add(Dense(4, activation='softmax'))

  model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])

  model.fit(X_train, y_train, epochs=10, batch_size=64)
  scores += model.evaluate(X_test, y_test, verbose=0)[1]
  #print(model.evaluate(X_test, y_test, verbose=0)[1])

print("Overall Accuracy: %.2f%%" % (100*scores/5))

"""##b) LSTM for Sequence Classification with Dropout (run on whole data set)"""

import time
start_time = time.time()
model = Sequential()
model.add(Embedding(MAX_NB_WORDS, EMBEDDING_DIM, input_length=X.shape[1]))

model.add(Dropout(0.4))
model.add(LSTM(100))
model.add(Dropout(0.4))
model.add(Dense(4, activation='softmax'))

model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
X_tra, X_te, y_tra, y_te = train_test_split(X , Y, test_size=0.2)

start_time = time.time()
#model.fit(X, Y, epochs=10, batch_size=64)
model.fit(X, Y, epochs=15, batch_size=128,validation_split=0.2)
#scores = cross_val_score(model, vectors_train_stem, train_numpy[:,1], cv=5)
# Final evaluation of the model

scores = model.evaluate(X_te, y_te, verbose=0)
print("Accuracy: %.2f%% Run Time: %.2f" % (scores[1]*100, time.time() - start_time))
print(f"Run Time: {time.time() - start_time} s")

pred = model.predict(X2)
pred_cat = []
for i in range(len(pred)):
  index = pred[i].argmax()
  if(int(index) == 0):
    pred_cat.append('Javascript')
  if (int(index) == 1):
    pred_cat.append('Matlab')
  if (int(index) == 2):
    pred_cat.append('Pytorch')
  if (int(index) == 3):
    pred_cat.append('Tensorflow')


print(pred_cat)
save_output(pred_cat)

"""###b-2) LSTM for Sequence Classification with Dropout (with cross validation)"""

from sklearn.model_selection import KFold


kf = KFold(n_splits=5, shuffle=True, random_state=50)
kf.get_n_splits(X)
scores = 0
start_time = time.time()
for train_index, test_index in kf.split(X):

  #print("TEST:", test_index)
  X_train, X_test = X[train_index], X[test_index]
  y_train, y_test = Y.loc[train_index], Y.loc[test_index]

  model = Sequential()
  model.add(Embedding(MAX_NB_WORDS, EMBEDDING_DIM, input_length=X.shape[1]))

  model.add(Dropout(0.4))
  model.add(LSTM(100))
  model.add(Dropout(0.4))
  model.add(Dense(4, activation='softmax'))

  model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])

  model.fit(X_train, y_train, epochs=15, batch_size=64)
  scores += model.evaluate(X_test, y_test, verbose=0)[1]
  #print(model.evaluate(X_test, y_test, verbose=0)[1])


print("Overall Accuracy: %.2f%%" % (100*scores/5))
print(f"Run Time: {time.time() - start_time} s")

"""##c) LSTM and Convolutional Neural Network for Sequence Classification (run on whole data set)"""

start_time = time.time()
model = Sequential()
model.add(Embedding(MAX_NB_WORDS, EMBEDDING_DIM, input_length=X.shape[1]))

model.add(Conv1D(filters=128, kernel_size=3, padding='same', activation='relu'))
model.add(MaxPooling1D(pool_size=6))

model.add(LSTM(100))
model.add(Dense(4, activation='softmax'))

model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
X_tra, X_te, y_tra, y_te = train_test_split(X , Y, test_size=0.2)


#model.fit(X, Y, epochs=10, batch_size=64)
model.fit(X, Y, epochs=15, batch_size=128,validation_split=0.2)
# Final evaluation of the model

scores = model.evaluate(X_te, y_te, verbose=0)
print("Accuracy: %.2f%% Run Time: %.2f s" % (scores[1]*100, time.time() - start_time))

pred = model.predict(X2)
pred_cat = []
for i in range(len(pred)):
  index = pred[i].argmax()
  if(int(index) == 0):
    pred_cat.append('Javascript')
  if (int(index) == 1):
    pred_cat.append('Matlab')
  if (int(index) == 2):
    pred_cat.append('Pytorch')
  if (int(index) == 3):
    pred_cat.append('Tensorflow')


print(pred_cat)

save_output(pred_cat)

"""###c) LSTM and Convolutional Neural Network for Sequence Classification (with cross validation)"""

start_time = time.time()

kf = KFold(n_splits=5, shuffle=True, random_state=50)
kf.get_n_splits(X)
scores = 0
for train_index, test_index in kf.split(X):

  #print("TEST:", test_index)
  X_train, X_test = X[train_index], X[test_index]
  y_train, y_test = Y.loc[train_index], Y.loc[test_index]


  model = Sequential()
  model.add(Embedding(MAX_NB_WORDS, EMBEDDING_DIM, input_length=X.shape[1]))

  model.add(Conv1D(filters=128, kernel_size=3, padding='same', activation='relu'))
  model.add(MaxPooling1D(pool_size=6))
  model.add(LSTM(100))

  model.add(Dense(4, activation='softmax'))

  model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])

  model.fit(X_train, y_train, epochs=15, batch_size=64)
  print(f"Run Time: {time.time() - start_time} s")
  scores += model.evaluate(X_test, y_test, verbose=0)[1]
  print(model.evaluate(X_test, y_test, verbose=0)[1])

print("Overall Accuracy: %.2f%%" % (100 * scores/5))

"""#4) Implement Bernoulli Naïve Bayes from scratch"""

from sklearn.feature_extraction.text import TfidfVectorizer
import collections

tf_idf_vectorizer = TfidfVectorizer(stop_words = 'english', binary=False, max_features=2000, lowercase=True, ngram_range=(1,1))
vectors_train_idf = tf_idf_vectorizer.fit_transform(train_numpy[:,0])
vectors_test_idf = tf_idf_vectorizer.transform(test_numpy[:,1])
vectors_train_idf= normalizer_train.transform(vectors_train_idf)
vectors_test_idf = normalizer_train.transform(vectors_test_idf)

Counter = [ [0] for i in range(4)]
for i in range(len(train_numpy[:,0])):
  if(train_numpy[i,1] == 'Javascript'):
    Counter[0] += vectors_train_idf.toarray()[i]
  if(train_numpy[i,1] == 'Matlab'):
    Counter[1] += vectors_train_idf.toarray()[i]
  if(train_numpy[i,1] == 'Pytorch'):
    Counter[2] += vectors_train_idf.toarray()[i]
  if(train_numpy[i,1] == 'Tensorflow'):
    Counter[3] += vectors_train_idf.toarray()[i]
Counter = np.array(Counter)


# Laplace smoothing
for i in range(4):
  for j in range(len(Counter[0])):
    if(Counter[i, j] == 0):
      Counter[i, j] = 1/(len(Counter[0])+2)
for i in range(4):
  Counter[i, :] = Counter[i, :] / Counter[i,:].sum()


counter = collections.Counter(train_numpy[:,1])
Theta = []
Theta.append( counter['Javascript']/vectors_train_idf.shape[0] )
Theta.append( counter['Matlab']/vectors_train_idf.shape[0] )
Theta.append( counter['Pytorch']/vectors_train_idf.shape[0] )
Theta.append( counter['Tensorflow']/vectors_train_idf.shape[0] )



F1 = []
F2 = []
F3 = []
F4 = []
prediction = []
for i in range(len(vectors_test_idf.toarray())):
  value0 = 0
  value1 = 0
  value2 = 0
  value3 = 0
  for k in range(len(vectors_test_idf.toarray()[0])):
    thetaj0 = Counter[0,k]
    thetaj1 = Counter[1,k]
    thetaj2 = Counter[2,k]
    thetaj3 = Counter[3,k]
    xj = vectors_test_idf.toarray()[i,k]
    value0 += xj * math.log(thetaj0) + (1-xj)* math.log(1-thetaj0)
    value1 += xj * math.log(thetaj1) + (1-xj)* math.log(1-thetaj1)
    value2 += xj * math.log(thetaj2) + (1-xj)* math.log(1-thetaj2)
    value3 += xj * math.log(thetaj3) + (1-xj)* math.log(1-thetaj3) 
  F1.append( math.log(Theta[0]) + value0)
  F2.append( math.log(Theta[1]) + value1)
  F3.append( math.log(Theta[2]) + value2)
  F4.append( math.log(Theta[3]) + value3)
  x = [math.log(Theta[0])+value0, math.log(Theta[1])+value1, math.log(Theta[2])+value2, math.log(Theta[3])+value3]
  prediction.append(x.index(max(x)))
      

    
pred = []
for i in prediction:
  if (int(i) == 0):
    pred.append('Javascript')
  if (int(i) == 1):
    pred.append('Matlab')
  if (int(i) == 2):
    pred.append('Pytorch')
  if (int(i) == 3):
    pred.append('Tensorflow')

print(pred)

"""The accuracy for our Bernoulli Naïve Bayes model on Kaggle is about 67% which is on par with sklearn model and TA's baseline."""

